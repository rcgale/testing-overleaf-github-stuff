@inproceedings{adams_target_2017,
 address = {Vancouver, Canada,},
 author = {Adams, Joel and Bedrick, Steven and Fergadiotis, Gerasimos and Gorman, Kyle and van Santen, Jan},
 booktitle = {{BioNLP} 2017},
 doi = {10.18653/v1/W17-2301},
 language = {en},
 pages = {1--8},
 publisher = {Association for Computational Linguistics},
 title = {Target word prediction and paraphasia classification in spoken discourse},
 url = {http://aclweb.org/anthology/W17-2301},
 urldate = {2022-10-17},
 year = {2017}
}

@inproceedings{balagopalan_bert_2020,
 author = {Balagopalan, Aparna and Eyre, Benjamin and Rudzicz, Frank and Novikova, Jekaterina},
 booktitle = {Interspeech 2020},
 doi = {10.21437/Interspeech.2020-2557},
 language = {en},
 month = {October},
 pages = {2167--2171},
 publisher = {ISCA},
 shorttitle = {To {BERT} or not to {BERT}},
 title = {To {BERT} or not to {BERT}: {Comparing} speech and language-based approaches for {Alzheimer}’s disease detection},
 url = {https://www.isca-speech.org/archive/interspeech_2020/balagopalan20_interspeech.html},
 urldate = {2023-02-13},
 year = {2020}
}

@incollection{bock_sentence_1995,
 author = {Bock, K},
 booktitle = {Speech, {Language}, and {Communication}},
 doi = {10.1016/B978-012497770-9/50008-X},
 isbn = {978-0-12-497770-9},
 language = {en},
 pages = {181--216},
 publisher = {Elsevier},
 title = {Sentence production: {From} mind to mouth},
 url = {https://linkinghub.elsevier.com/retrieve/pii/B978012497770950008X},
 urldate = {2022-12-14},
 year = {1995}
}

@article{breimaier_consolidated_2015,
 author = {Breimaier, Helga E. and Heckemann, Birgit and Halfens, Ruud J. G. and Lohrmann, Christa},
 doi = {10.1186/s12912-015-0088-4},
 issn = {1472-6955},
 journal = {BMC Nursing},
 language = {en},
 month = {December},
 number = {1},
 pages = {43},
 shorttitle = {The {Consolidated} {Framework} for {Implementation} {Research} ({CFIR})},
 title = {The {Consolidated} {Framework} for {Implementation} {Research} ({CFIR}): {A} useful theoretical framework for guiding and evaluating a guideline implementation process in a hospital-based nursing practice},
 url = {http://bmcnurs.biomedcentral.com/articles/10.1186/s12912-015-0088-4},
 urldate = {2022-12-14},
 volume = {14},
 year = {2015}
}

@article{bryant_propositional_2013,
 author = {Bryant, Lucy and Spencer, Elizabeth and Ferguson, Alison and Craig, Hugh and Colyvas, Kim and Worrall, Linda},
 doi = {10.1080/02687038.2013.803514},
 issn = {0268-7038, 1464-5041},
 journal = {Aphasiology},
 language = {en},
 month = {August},
 number = {8},
 pages = {992--1009},
 title = {Propositional {Idea} {Density} in aphasic discourse},
 url = {http://www.tandfonline.com/doi/abs/10.1080/02687038.2013.803514},
 urldate = {2022-12-14},
 volume = {27},
 year = {2013}
}

@article{butterworth_paragrammatisms_1987,
 author = {Butterworth, Brian and Howard, David},
 doi = {10.1016/0010-0277(87)90012-6},
 issn = {00100277},
 journal = {Cognition},
 language = {en},
 month = {June},
 number = {1},
 pages = {1--37},
 title = {Paragrammatisms},
 url = {https://linkinghub.elsevier.com/retrieve/pii/0010027787900126},
 urldate = {2023-01-27},
 volume = {26},
 year = {1987}
}

@article{casilio_paralg_2023,
 author = {Casilio, Marianne and Fergadiotis, Gerasimos and Salem, Alexandra C. and Gale, Robert and McKinney-Bock, Katy and Bedrick, Steven},
 doi = {10.1044/2022_JSLHR-22-00255},
 journal = {Journal of Speech, Language, and Hearing Research},
 title = {{ParAlg}: {A} paraphasia algorithm for multinomial classification of picture naming errors},
 year = {2023}
}

@inproceedings{chatzoudis_zero-shot_2022,
 author = {Chatzoudis, Gerasimos and Plitsis, Manos and Stamouli, Spyridoula and Dimou, Athanasia–Lida and Katsamanis, Nassos and Katsouros, Vassilis},
 booktitle = {Interspeech 2022},
 doi = {10.21437/Interspeech.2022-10681},
 language = {en},
 month = {September},
 pages = {2178--2182},
 publisher = {ISCA},
 title = {Zero-shot cross-lingual aphasia detection using automatic speech recognition},
 url = {https://www.isca-speech.org/archive/interspeech_2022/chatzoudis22_interspeech.html},
 urldate = {2023-02-13},
 year = {2022}
}

@article{cruice_finding_2003,
 author = {Cruice, Madeline and Worrall, Linda and Hickson, Louise and Murison, Robert},
 doi = {10.1080/02687030244000707},
 issn = {0268-7038, 1464-5041},
 journal = {Aphasiology},
 language = {en},
 month = {January},
 number = {4},
 pages = {333--353},
 shorttitle = {Finding a focus for quality of life with aphasia},
 title = {Finding a focus for quality of life with aphasia: {Social} and emotional health, and psychological well-being},
 url = {https://www.tandfonline.com/doi/full/10.1080/02687030244000707},
 urldate = {2022-12-14},
 volume = {17},
 year = {2003}
}

@article{damschroder_fostering_2009,
 author = {Damschroder, Laura J and Aron, David C and Keith, Rosalind E and Kirsh, Susan R and Alexander, Jeffery A and Lowery, Julie C},
 doi = {10.1186/1748-5908-4-50},
 issn = {1748-5908},
 journal = {Implementation Science},
 language = {en},
 month = {December},
 number = {1},
 pages = {50},
 shorttitle = {Fostering implementation of health services research findings into practice},
 title = {Fostering implementation of health services research findings into practice: a consolidated framework for advancing implementation science},
 url = {http://implementationscience.biomedcentral.com/articles/10.1186/1748-5908-4-50},
 urldate = {2022-12-14},
 volume = {4},
 year = {2009}
}

@inproceedings{day_predicting_2021,
 address = {Mexico},
 author = {Day, Marjory and Dey, Rupam Kumar and Baucum, Matthew and Paek, Eun Jin and Park, Hyejin and Khojandi, Anahita},
 booktitle = {2021 43rd {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} \& {Biology} {Society} ({EMBC})},
 doi = {10.1109/EMBC46164.2021.9630694},
 isbn = {978-1-72811-179-7},
 month = {November},
 pages = {2299--2302},
 publisher = {IEEE},
 shorttitle = {Predicting severity in people with aphasia},
 title = {Predicting severity in people with aphasia: {A} natural language processing and machine learning approach},
 url = {https://ieeexplore.ieee.org/document/9630694/},
 urldate = {2022-12-14},
 year = {2021}
}

@article{dell_connectionist_1999,
 author = {Dell, Gary S. and Chang, Franklin and Griffin, Zenzi M.},
 doi = {10.1207/s15516709cog2304_6},
 issn = {03640213},
 journal = {Cognitive Science},
 language = {en},
 month = {October},
 number = {4},
 pages = {517--542},
 shorttitle = {Connectionist models of language production},
 title = {Connectionist models of language production: {Lexical} access and grammatical encoding},
 url = {http://doi.wiley.com/10.1207/s15516709cog2304_6},
 urldate = {2022-12-14},
 volume = {23},
 year = {1999}
}

@article{dell_spreading-activation_1986,
 author = {Dell, Gary S.},
 doi = {10.1037/0033-295X.93.3.283},
 issn = {1939-1471, 0033-295X},
 journal = {Psychological Review},
 language = {en},
 number = {3},
 pages = {283--321},
 title = {A spreading-activation theory of retrieval in sentence production.},
 url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.93.3.283},
 urldate = {2022-12-14},
 volume = {93},
 year = {1986}
}

@inproceedings{devlin_bert_2019,
 abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
 doi = {10.18653/v1/N19-1423},
 month = {June},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 shorttitle = {Bert},
 title = {{BERT}: {Pre}-training of deep bidirectional transformers for language understanding},
 url = {https://aclanthology.org/N19-1423},
 urldate = {2022-03-23},
 year = {2019}
}

@article{doi:10.1073/pnas.1903070116,
 abstract = {While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
 author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
 doi = {10.1073/pnas.1903070116},
 journal = {Proceedings of the National Academy of Sciences},
 note = {tex.date-added: 2022-06-03T22:01:15GMT
tex.date-modified: 2022-07-20T00:21:56GMT
tex.local-url: file://localhost/Users/bedricks/Dropbox/Papers\%203\%20On\%20Dropbox/Library.papers3/Files/91/91B6ADDA-18A6-4260-B8CB-18A78E7B87B0.pdf
tex.rating: 0
tex.read: Yes
tex.uri: papers3://publication/doi/10.1073/pnas.1903070116},
 number = {32},
 pages = {15849--15854},
 title = {Reconciling modern machine-learning practice and the classical bias-variance trade-off},
 url = {https://www.pnas.org/doi/abs/10.1073/pnas.1903070116},
 volume = {116},
 year = {2019}
}

@article{doi:10.1162/neco.1992.4.1.1,
 abstract = {Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.},
 author = {Geman, Stuart and Bienenstock, Elie and Doursat, René},
 doi = {10.1162/neco.1992.4.1.1},
 journal = {Neural Computation},
 language = {English},
 note = {tex.date-added: 2019-12-31T18:24:08GMT
tex.date-modified: 2022-07-20T00:21:54GMT
tex.local-url: file://localhost/Users/bedricks/Dropbox/Papers\%203\%20On\%20Dropbox/Library.papers3/Files/A3/A34DDBBF-9924-469C-8D9D-B3EE86E48744.pdf
tex.rating: 0
tex.read: Yes
tex.uri: papers3://publication/doi/10.1162/neco.1992.4.1.1},
 number = {1},
 pages = {1--58},
 title = {Neural networks and the {Bias}/{Variance} dilemma},
 url = {https://doi.org/10.1162/neco.1992.4.1.1},
 volume = {4},
 year = {1992}
}

@inproceedings{feng-etal-2021-survey,
 address = {Online},
 author = {Feng, Steven Y. and Gangal, Varun and Wei, Jason and Chandar, Sarath and Vosoughi, Soroush and Mitamura, Teruko and Hovy, Eduard},
 booktitle = {Findings of the association for computational linguistics: {ACL}-{IJCNLP} 2021},
 doi = {10.18653/v1/2021.findings-acl.84},
 month = {August},
 pages = {968--988},
 publisher = {Association for Computational Linguistics},
 title = {A survey of data augmentation approaches for {NLP}},
 url = {https://aclanthology.org/2021.findings-acl.84},
 year = {2021}
}

@article{fergadiotis_algorithmic_2016,
 abstract = {Purpose
This study was intended to evaluate a series of algorithms developed to perform automatic classification of paraphasic errors (formal, semantic, mixed, neologistic, and unrelated errors).


Method
We analyzed 7,111 paraphasias from the Moss Aphasia Psycholinguistics Project Database (Mirman et al., 2010) and evaluated the classification accuracy of 3 automated tools. First, we used frequency norms from the SUBTLEXus database (Brysbaert \& New, 2009) to differentiate nonword errors and real-word productions. Then we implemented a phonological-similarity algorithm to identify phonologically related real-word errors. Last, we assessed the performance of a semantic-similarity criterion that was based on word2vec (Mikolov, Yih, \& Zweig, 2013).


Results
Overall, the algorithmic classification replicated human scoring for the major categories of paraphasias studied with high accuracy. The tool that was based on the SUBTLEXus frequency norms was more than 97\% accurate in making lexicality judgments. The phonological-similarity criterion was approximately 91\% accurate, and the overall classification accuracy of the semantic classifier ranged from 86\% to 90\%.


Conclusion
Overall, the results highlight the potential of tools from the field of natural language processing for the development of highly reliable, cost-effective diagnostic tools suitable for collecting high-quality measurement data for research and clinical purposes.},
 author = {Fergadiotis, Gerasimos and Gorman, Kyle and Bedrick, Steven},
 doi = {10.1044/2016_AJSLP-15-0147},
 issn = {1058-0360, 1558-9110},
 journal = {American Journal of Speech-Language Pathology},
 language = {en},
 month = {December},
 number = {4S},
 title = {Algorithmic classification of five characteristic types of paraphasias},
 url = {http://pubs.asha.org/doi/10.1044/2016_AJSLP-15-0147},
 urldate = {2021-05-14},
 volume = {25},
 year = {2016}
}

@article{fergadiotis_lexical_2011,
 author = {Fergadiotis, Gerasimos and Wright, Heather Harris},
 doi = {10.1080/02687038.2011.603898},
 issn = {0268-7038, 1464-5041},
 journal = {Aphasiology},
 language = {en},
 month = {November},
 number = {11},
 pages = {1414--1430},
 title = {Lexical diversity for adults with and without aphasia across discourse elicitation tasks},
 url = {https://www.tandfonline.com/doi/full/10.1080/02687038.2011.603898},
 urldate = {2022-12-14},
 volume = {25},
 year = {2011}
}

@article{fergadiotis_measuring_2013,
 abstract = {Purpose
A microlinguistic content analysis for assessing lexical semantics in people with aphasia (PWA) is lexical diversity (LD). Sophisticated techniques have been developed to measure LD. However, validity evidence for these methodologies when applied to the discourse of PWA is lacking. The purpose of this study was to evaluate four measures of LD to determine how effective they were at measuring LD in PWA.


Method
Four measures of LD were applied to short discourse samples produced by 101 PWA: (a) the Measure of Textual Lexical Diversity (MTLD; McCarthy, 2005), (b) the Moving-Average Type-Token Ratio (MATTR; Covington, 2007), (c) D (McKee, Malvern, \& Richards, 2000), and (d) the Hypergeometric Distribution (HD-D; McCarthy \& Jarvis, 2007). LD was estimated using each method, and the scores were subjected to a series of analyses (e.g., curve-fitting, analysis of variance, confirmatory factor analysis).


Results
Results from the confirmatory factor analysis suggested that MTLD and MATTR reflect LD and little of anything else. Further, two indices (HD-D and D) were found to be equivalent, suggesting that either one can be used when samples are {\textgreater}50 tokens.


Conclusion
MTLD and MATTR yielded the strongest evidence for producing unbiased LD scores, suggesting that they may be the best measures for capturing LD in PWA.},
 author = {Fergadiotis, Gerasimos and Wright, Heather H. and West, Thomas M.},
 doi = {10.1044/1058-0360(2013/12-0083)},
 issn = {1058-0360, 1558-9110},
 journal = {American Journal of Speech-Language Pathology},
 language = {en},
 month = {May},
 number = {2},
 title = {Measuring lexical diversity in narrative discourse of people with aphasia},
 url = {http://pubs.asha.org/doi/10.1044/1058-0360%282013/12-0083%29},
 urldate = {2022-12-14},
 volume = {22},
 year = {2013}
}

@article{fergadiotis_modeling_2019,
 author = {Fergadiotis, Gerasimos and Kapantzoglou, Maria and Kintz, Stephen and Wright, Heather Harris},
 doi = {10.1080/02687038.2018.1482404},
 issn = {0268-7038, 1464-5041},
 journal = {Aphasiology},
 language = {en},
 month = {May},
 number = {5},
 pages = {544--560},
 title = {Modeling confrontation naming and discourse informativeness using structural equation modeling},
 url = {https://www.tandfonline.com/doi/full/10.1080/02687038.2018.1482404},
 urldate = {2023-01-27},
 volume = {33},
 year = {2019}
}

@article{fergadiotis_productive_2011,
 author = {Fergadiotis, Gerasimos and Wright, Heather Harris and Capilouto, Gilson J.},
 doi = {10.1080/02687038.2011.606974},
 issn = {0268-7038, 1464-5041},
 journal = {Aphasiology},
 language = {en},
 month = {October},
 number = {10},
 pages = {1261--1278},
 title = {Productive vocabulary across discourse types},
 url = {https://www.tandfonline.com/doi/full/10.1080/02687038.2011.606974},
 urldate = {2022-12-14},
 volume = {25},
 year = {2011}
}

@inproceedings{forbes_eval_2014,
 address = {St. Simons Island, GA},
 author = {Forbes, Margaret and Fromm, Davida and Holland, Audrey and MacWhinney, Brian},
 title = {{EVAL}: {A} tool for clinicians from {AphasiaBank}},
 year = {2014}
}

@inproceedings{fraser-etal-2013-automatic,
 address = {Grenoble, France},
 author = {Fraser, Kathleen and Rudzicz, Frank and Graham, Naida and Rochon, Elizabeth},
 booktitle = {Proceedings of the fourth workshop on speech and language processing for assistive technologies},
 month = {August},
 note = {tex.date-added: 2020-10-02T20:48:40GMT
tex.date-modified: 2021-07-03T01:15:05GMT
tex.local-url: file://localhost/Users/bedricks/Dropbox/Papers\%203\%20On\%20Dropbox/Library.papers3/Files/14/149DB1D5-9899-4921-892A-7D0CDE395EB9.pdf
tex.rating: 0
tex.uri: papers3://publication/uuid/24E228EB-BECC-4E31-9A74-397E91E3F7B6},
 pages = {47--54},
 publisher = {Association for Computational Linguistics},
 title = {Automatic speech recognition in the diagnosis of primary progressive aphasia},
 url = {https://www.aclweb.org/anthology/W13-3909},
 year = {2013}
}

@article{gale_automated_2021,
 abstract = {Speech and language impairments are common pediatric conditions, with as many as 10\% of children experiencing one or both at some point during development. Expressive language disorders in particular often go undiagnosed, underscoring the immediate need for assessments of expressive language that can be administered and scored reliably and objectively. In this paper, we present a set of highly accurate computational models for automatically scoring several common expressive language tasks. In our assessment framework, instructions and stimuli are presented to the child on a tablet computer, which records the child's responses in real time, while a clinician controls the pace and presentation of the tasks using a second tablet. The recorded responses for four distinct expressive language tasks (expressive vocabulary, word structure, recalling sentences, and formulated sentences) are then scored using traditional paper-and-pencil scoring and using machine learning methods relying on a deep neural network-based language representation model. All four tasks can be scored automatically from both clean and verbatim speech transcripts with very high accuracy at the item level (83−99\%). In addition, these automated scores correlate strongly and significantly (ρ = 0.76–0.99, 
p 
\&lt; 0.001) with manual item-level, raw, and scaled scores. These results point to the utility and potential of automated computationally-driven methods of both administering and scoring expressive language tasks for pediatric developmental language evaluation.},
 author = {Gale, Robert and Bird, Julie and Wang, Yiyi and van Santen, Jan and Prud'hommeaux, Emily and Dolata, Jill and Asgari, Meysam},
 doi = {10.3389/fpsyg.2021.668401},
 issn = {1664-1078},
 journal = {Frontiers in Psychology},
 month = {July},
 pages = {668401},
 title = {Automated scoring of tablet-administered expressive language tests},
 url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.668401/full},
 urldate = {2022-11-30},
 volume = {12},
 year = {2021}
}

@inproceedings{gale_post-stroke_2022,
 abstract = {We present the outcome of the Post-Stroke Speech Transcription (PSST) challenge. For the challenge, we prepared a new data resource of responses to two confrontation naming tests found in AphasiaBank, extracting audio and adding new phonemic transcripts for each response. The challenge consisted of two tasks. Task A asked challengers to build an automatic speech recognizer (ASR) for phonemic transcription of the PSST samples, evaluated in terms of phoneme error rate (PER) as well as a finer-grained metric derived from phonological feature theory, feature error rate (FER). The best model had a 9.9\% FER / 20.0\% PER, improving on our baseline by a relative 18\% and 24\%, respectively. Task B approximated a downstream assessment task, asking challengers to identify whether each recording contained a correctly pronounced target word. Challengers were unable to improve on the baseline algorithm; however, using this algorithm with the improved transcripts from Task A resulted in 92.8\% accuracy / 0.921 F1, a relative improvement of 2.8\% and 3.3\%, respectively.},
 address = {Marseille, France},
 author = {Gale, Robert C. and Fleegle, Mikala and Fergadiotis, Gerasimos and Bedrick, Steven},
 booktitle = {Proceedings of the {RaPID} {Workshop} - {Resources} and {ProcessIng} of linguistic, para-linguistic and extra-linguistic {Data} from people with various forms of cognitive/psychiatric/developmental impairments - within the 13th {Language} {Resources} and {Evaluation} {Conference}},
 month = {June},
 pages = {41--55},
 publisher = {European Language Resources Association},
 title = {The {Post}-{Stroke} {Speech} {Transcription} ({PSST}) challenge},
 url = {https://aclanthology.org/2022.rapid-1.6},
 year = {2022}
}

@book{goodglass_anomia_1997,
 address = {San Diego},
 editor = {Goodglass, Harold and Wingfield, Arthur},
 isbn = {978-0-12-289685-9},
 keywords = {Anomia},
 publisher = {Academic Press},
 series = {Foundations of neuropsychology},
 shorttitle = {Anomia},
 title = {Anomia: neuroanatomical and cognitive correlates},
 year = {1997}
}

@book{goodglass_understanding_1993,
 address = {San Diego},
 author = {Goodglass, Harold},
 isbn = {978-0-12-290040-2},
 keywords = {Aphasia},
 publisher = {Academic Press},
 series = {Foundations of neuropsychology series},
 title = {Understanding aphasia},
 year = {1993}
}

@article{hickin_treatment_2001,
 author = {Hickin, Julie and Best, Wendy and Herbert, Ruth and Howard, David and Osborne, Felicity},
 doi = {10.3109/13682820109177851},
 issn = {1368-2822, 1460-6984},
 journal = {International Journal of Language \& Communication Disorders},
 language = {en},
 month = {January},
 number = {s1},
 pages = {13--18},
 shorttitle = {Treatment of word retrieval in aphasia},
 title = {Treatment of word retrieval in aphasia: {Generalisation} to conversational speech},
 url = {http://doi.wiley.com/10.3109/13682820109177851},
 urldate = {2022-12-14},
 volume = {36},
 year = {2001}
}

@misc{kertesz_western_2012,
 author = {Kertesz, Andrew},
 doi = {10.1037/t15168-000},
 language = {en},
 month = {November},
 publisher = {American Psychological Association},
 title = {Western {Aphasia} {Battery}--{Revised}},
 url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/t15168-000},
 urldate = {2023-01-18},
 year = {2012}
}

@inproceedings{kudo_sentencepiece_2018,
 address = {Brussels, Belgium},
 author = {Kudo, Taku and Richardson, John},
 booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
 doi = {10.18653/v1/D18-2012},
 language = {en},
 pages = {66--71},
 publisher = {Association for Computational Linguistics},
 shorttitle = {{SentencePiece}},
 title = {{SentencePiece}: {A} simple and language independent subword tokenizer and detokenizer for {Neural} {Text} {Processing}},
 url = {http://aclweb.org/anthology/D18-2012},
 urldate = {2023-02-09},
 year = {2018}
}

@inproceedings{Le+2016,
 author = {Le, Duc and Provost, Emily Mower},
 booktitle = {Interspeech 2016},
 doi = {10.21437/Interspeech.2016-213},
 note = {tex.date-added: 2020-10-02T20:51:43GMT
tex.date-modified: 2021-05-24T03:48:24GMT
tex.local-url: file://localhost/Users/bedricks/Dropbox/Papers\%203\%20On\%20Dropbox/Library.papers3/Files/0E/0E167C49-45A6-45A3-B1CE-3687BD854DD5.pdf
tex.rating: 0
tex.read: Yes
tex.uri: papers3://publication/doi/10.21437/Interspeech.2016-213},
 pages = {2681--2685},
 publisher = {ISCA},
 title = {Improving automatic recognition of aphasic speech with {AphasiaBank}},
 url = {http://dx.doi.org/10.21437/Interspeech.2016-213},
 year = {2016}
}

@article{LE20181,
 abstract = {Spontaneous speech analysis plays an important role in the study and treatment of aphasia, but can be difficult to perform manually due to the time consuming nature of speech transcription and coding. Techniques in automatic speech recognition and assessment can potentially alleviate this problem by allowing clinicians to quickly process large amount of speech data. However, automatic analysis of spontaneous aphasic speech has been relatively under-explored in the engineering literature, partly due to the limited amount of available data and difficulties associated with aphasic speech processing. In this work, we perform one of the first large-scale quantitative analysis of spontaneous aphasic speech based on automatic speech recognition (ASR) output. We describe our acoustic modeling method that sets a new recognition benchmark on AphasiaBank, a large-scale aphasic speech corpus. We propose a set of clinically-relevant quantitative measures that are shown to be highly robust to automatic transcription errors. Finally, we demonstrate that these measures can be used to accurately predict the revised Western Aphasia Battery (WAB-R) Aphasia Quotient (AQ) without the need for manual transcripts. The results and techniques presented in our work will help advance the state-of-the-art in aphasic speech processing and make ASR-based technology for aphasia treatment more feasible in real-world clinical applications.},
 author = {Le, Duc and Licata, Keli and Mower Provost, Emily},
 doi = {https://doi.org/10.1016/j.specom.2018.04.001},
 journal = {Speech communication},
 note = {tex.date-added: 2020-10-02T20:50:41GMT
tex.date-modified: 2021-05-23T17:29:43GMT
tex.local-url: file://localhost/Users/bedricks/Dropbox/Papers\%203\%20On\%20Dropbox/Library.papers3/Files/A9/A9BDB7CC-A87F-49D2-A618-C76791EB243F.pdf
tex.rating: 0
tex.uri: papers3://publication/doi/https://doi.org/10.1016/j.specom.2018.04.001},
 pages = {1--12},
 title = {Automatic quantitative analysis of spontaneous aphasic speech},
 url = {http://www.sciencedirect.com/science/article/pii/S0167639317303126},
 volume = {100},
 year = {2018}
}

@inproceedings{le_automatic_2017,
 author = {Le, Duc and Licata, Keli and Provost, Emily Mower},
 booktitle = {Proc. {Interspeech} 2017},
 doi = {10.21437/Interspeech.2017-626},
 pages = {294--298},
 title = {Automatic paraphasia detection from aphasic speech: {A} preliminary study},
 url = {http://dx.doi.org/10.21437/Interspeech.2017-626},
 year = {2017}
}

@article{levelt_models_1999,
 author = {Levelt, Willem J.M.},
 doi = {10.1016/S1364-6613(99)01319-4},
 issn = {13646613},
 journal = {Trends in Cognitive Sciences},
 language = {en},
 month = {June},
 number = {6},
 pages = {223--232},
 title = {Models of word production},
 url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661399013194},
 urldate = {2022-12-14},
 volume = {3},
 year = {1999}
}

@article{levelt_theory_1999,
 author = {Levelt, Willem J. M. and Roelofs, Ardi and Meyer, Antje S.},
 doi = {10.1017/S0140525X99001776},
 issn = {0140-525X, 1469-1825},
 journal = {Behavioral and Brain Sciences},
 language = {en},
 month = {February},
 number = {01},
 title = {A theory of lexical access in speech production},
 url = {http://www.journals.cambridge.org/abstract_S0140525X99001776},
 urldate = {2022-12-14},
 volume = {22},
 year = {1999}
}

@inproceedings{liu_text_2019,
 address = {Hong Kong, China},
 author = {Liu, Yang and Lapata, Mirella},
 booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
 doi = {10.18653/v1/D19-1387},
 language = {en},
 pages = {3728--3738},
 publisher = {Association for Computational Linguistics},
 title = {Text summarization with pretrained encoders},
 url = {https://www.aclweb.org/anthology/D19-1387},
 urldate = {2022-03-22},
 year = {2019}
}

@phdthesis{lowerre_harpy_1976,
 abstract = {Computer Science Department},
 author = {Lowerre, T. B.},
 keywords = {89999 Information and Computing Sciences not elsewhere classified, FOS: Computer and information sciences},
 school = {Carnegie Mellon University},
 title = {The {Harpy} speech recognition system},
 type = {Ph.{D}. {Thesis}},
 year = {1976}
}

@article{macdonald_lexical_1994,
 author = {MacDonald, Maryellen C. and Pearlmutter, Neal J. and Seidenberg, Mark S.},
 doi = {10.1037/0033-295X.101.4.676},
 issn = {1939-1471, 0033-295X},
 journal = {Psychological Review},
 language = {en},
 number = {4},
 pages = {676--703},
 title = {The lexical nature of syntactic ambiguity resolution.},
 url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.101.4.676},
 urldate = {2022-12-14},
 volume = {101},
 year = {1994}
}

@article{macwhinney_aphasiabank_2011,
 author = {MacWhinney, Brian and Fromm, Davida and Forbes, Margaret and Holland, Audrey},
 doi = {10.1080/02687038.2011.589893},
 issn = {0268-7038, 1464-5041},
 journal = {Aphasiology},
 language = {en},
 month = {November},
 number = {11},
 pages = {1286--1307},
 shorttitle = {Aphasiabank},
 title = {Aphasiabank: {Methods} for studying discourse},
 url = {https://www.tandfonline.com/doi/full/10.1080/02687038.2011.589893},
 urldate = {2022-12-14},
 volume = {25},
 year = {2011}
}

@book{macwhinney_childes_2000,
 address = {Mahwah, NJ},
 author = {MacWhinney, B},
 edition = {3rd},
 publisher = {Lawrence Erlbaum Associates},
 title = {The {CHILDES} project: {Tools} for analyzing talk},
 year = {2000}
}

@book{malvern_lexical_2008,
 address = {Basingstoke},
 editor = {Malvern, David},
 isbn = {978-1-4039-0232-0 978-1-4039-0231-3},
 language = {eng},
 publisher = {Palgrave Macmillan},
 shorttitle = {Lexical diversity and language development},
 title = {Lexical diversity and language development: quantification and assessment},
 year = {2008}
}

@article{mayer_functional_2003,
 author = {Mayer, Jamie and Murray, Laura},
 doi = {10.1080/02687030344000148},
 issn = {0268-7038, 1464-5041},
 journal = {Aphasiology},
 language = {en},
 month = {January},
 number = {5},
 pages = {481--497},
 shorttitle = {Functional measures of naming in aphasia},
 title = {Functional measures of naming in aphasia: {Word} retrieval in confrontation naming versus connected speech},
 url = {https://www.tandfonline.com/doi/full/10.1080/02687030344000148},
 urldate = {2022-12-14},
 volume = {17},
 year = {2003}
}

@article{mcnemar_note_1947,
 author = {McNemar, Quinn},
 doi = {10.1007/BF02295996},
 issn = {0033-3123, 1860-0980},
 journal = {Psychometrika},
 language = {en},
 month = {June},
 number = {2},
 pages = {153--157},
 title = {Note on the sampling error of the difference between correlated proportions or percentages},
 url = {http://link.springer.com/10.1007/BF02295996},
 urldate = {2023-01-20},
 volume = {12},
 year = {1947}
}

@misc{miller_systematic_2012,
 address = {Middleton, WI},
 author = {Miller, Jon and Iglesias, Aquiles},
 publisher = {SALT Software, LLC},
 title = {Systematic {Analysis} of {Language} {Transcripts} ({SALT}), research version 2012 [computer software]},
 year = {2012}
}

@inproceedings{pai_unsupervised_2020,
 abstract = {Aphasia is a speech and language disorder which results from brain damage, often characterized by word retrieval deficit (anomia) resulting in naming errors (paraphasia). Automatic paraphasia detection has many benefits for both treatment and diagnosis of Aphasia and its type. But supervised learning methods cant be properly utilized as there is a lack of aphasic speech data. In this paper, we describe our novel unsupervised method which can be implemented without the need for labeled paraphasia data. Our evaluations show that our method outperforms previous work based on supervised learning and transfer learning approaches for English. We demonstrate the utility of our method as an essential first step in developing augmentative and alternative communication (AAC) devices for patients suffering from aphasia in any language.},
 address = {Online},
 author = {Pai, Sharan and Sachdeva, Nikhil and Sachdeva, Prince and Shah, Rajiv Ratn},
 booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Student} {Research} {Workshop}},
 doi = {10.18653/v1/2020.acl-srw.3},
 month = {July},
 pages = {13--19},
 publisher = {Association for Computational Linguistics},
 title = {Unsupervised paraphasia classification in aphasic speech},
 url = {https://aclanthology.org/2020.acl-srw.3},
 year = {2020}
}

@incollection{papathanasiou_disorders_2017,
 author = {Papathanasiou, Ilias and Coppens, Patrick},
 booktitle = {Aphasia {And} {Related} {Neurogenic} {Communication} {Disorders}},
 edition = {1st},
 pages = {169--195},
 publisher = {Jones \& Bartlett Learning},
 title = {Disorders of word production},
 year = {2017}
}

@article{pashek_context_2002,
 author = {Pashek, Gail V. and Tompkins, Connie A.},
 doi = {10.1080/02687040143000573},
 issn = {0268-7038, 1464-5041},
 journal = {Aphasiology},
 language = {en},
 month = {March},
 number = {3},
 pages = {261--286},
 title = {Context and word class influences on lexical retrieval in aphasia},
 url = {http://www.tandfonline.com/doi/abs/10.1080/02687040143000573},
 urldate = {2022-12-14},
 volume = {16},
 year = {2002}
}

@inproceedings{peng_transfer_2019,
 address = {Florence, Italy},
 author = {Peng, Yifan and Yan, Shankai and Lu, Zhiyong},
 booktitle = {Proceedings of the 18th {BioNLP} {Workshop} and {Shared} {Task}},
 doi = {10.18653/v1/W19-5006},
 language = {en},
 pages = {58--65},
 publisher = {Association for Computational Linguistics},
 shorttitle = {Transfer {Learning} in {Biomedical} {Natural} {Language} {Processing}},
 title = {Transfer learning in biomedical natural language processing: {An} evaluation of {BERT} and {ELMo} on ten benchmarking datasets},
 url = {https://www.aclweb.org/anthology/W19-5006},
 urldate = {2023-02-13},
 year = {2019}
}

@inproceedings{perez_aphasic_2020,
 author = {Perez, Matthew and Aldeneh, Zakaria and Provost, Emily Mower},
 booktitle = {Interspeech 2020},
 doi = {10.21437/Interspeech.2020-2049},
 language = {en},
 month = {October},
 pages = {4986--4990},
 publisher = {ISCA},
 title = {Aphasic speech recognition using a mixture of speech intelligibility experts},
 url = {https://www.isca-speech.org/archive/interspeech_2020/perez20_interspeech.html},
 urldate = {2023-02-01},
 year = {2020}
}

@article{rabin_assessment_2005,
 author = {Rabin, L and Barr, W and Burton, L},
 doi = {10.1016/j.acn.2004.02.005},
 issn = {08876177},
 journal = {Archives of Clinical Neuropsychology},
 language = {en},
 month = {January},
 number = {1},
 pages = {33--65},
 shorttitle = {Assessment practices of clinical neuropsychologists in the {United} {States} and {Canada}},
 title = {Assessment practices of clinical neuropsychologists in the {United} {States} and {Canada}: {A} survey of {INS}, {NAN}, and {APA} {Division} 40 members},
 url = {https://academic.oup.com/acn/article-lookup/doi/10.1016/j.acn.2004.02.005},
 urldate = {2023-01-27},
 volume = {20},
 year = {2005}
}

@article{richardson_relationship_2018,
 abstract = {Purpose
The purpose of this study was to examine the relationship between picture naming performance and the ability to communicate the gist, or essential elements, of a story. We also sought to determine if this relationship varied according to Western Aphasia Battery–Revised (WAB-R; Kertesz, 2007) aphasia subtype.


Method
Demographic information, test scores, and transcripts of 258 individuals with aphasia completing 3 narrative tasks were retrieved from the AphasiaBank database. Narratives were subjected to a main concept analysis to determine gist production. A correlation analysis was used to investigate the relationship between naming scores and main concept production for the whole group of persons with aphasia and for WAB-R subtypes separately.


Results
We found strong correlations between naming test scores and narrative gist production for the large sample of persons with aphasia. However, the strength of the correlations varied by WAB-R subtype.


Conclusions
Picture naming may accurately predict gist production for individuals with Broca's and Wernicke's aphasia, but not for other WAB-R subtypes. Given the current reprioritization of outcome measurement, picture naming may not be an appropriate surrogate measure for functional communication for all persons with aphasia.


Supplemental Materials

https://doi.org/10.23641/asha.5851848},
 author = {Richardson, Jessica D. and Hudspeth Dalton, Sarah Grace and Fromm, Davida and Forbes, Margaret and Holland, Audrey and MacWhinney, Brian},
 doi = {10.1044/2017_AJSLP-16-0211},
 issn = {1058-0360, 1558-9110},
 journal = {American Journal of Speech-Language Pathology},
 language = {en},
 month = {March},
 number = {1S},
 pages = {406--422},
 title = {The relationship between confrontation naming and story gist production in aphasia},
 url = {http://pubs.asha.org/doi/10.1044/2017_AJSLP-16-0211},
 urldate = {2023-01-27},
 volume = {27},
 year = {2018}
}

@article{roach_philadelphia_1996,
 author = {Roach, A. and Schwartz, M. F. and Martin, N. and Grewal, R. S. and Brecher, A.},
 journal = {Clinical Aphasiology},
 pages = {121--133},
 title = {The {Philadelphia} {Naming} {Test}: {Scoring} and rationale},
 volume = {24},
 year = {1996}
}

@article{saffran_quantitative_1989,
 author = {Saffran, Eleanor M. and Berndt, Rita Sloan and Schwartz, Myrna F.},
 doi = {10.1016/0093-934X(89)90030-8},
 issn = {0093934X},
 journal = {Brain and Language},
 language = {en},
 month = {October},
 number = {3},
 pages = {440--479},
 shorttitle = {The quantitative analysis of agrammatic production},
 title = {The quantitative analysis of agrammatic production: {Procedure} and data},
 url = {https://linkinghub.elsevier.com/retrieve/pii/0093934X89900308},
 urldate = {2023-01-27},
 volume = {37},
 year = {1989}
}

@article{salem_refining_2022,
 abstract = {Purpose: 
ParAlg (Paraphasia Algorithms) is a software that automatically categorizes a person with aphasia's naming error (paraphasia) in relation to its intended target on a picture-naming test. These classifications (based on lexicality as well as semantic, phonological, and morphological similarity to the target) are important for characterizing an individual's word-finding deficits or anomia. In this study, we applied a modern language model called BERT (Bidirectional Encoder Representations from Transformers) as a semantic classifier and evaluated its performance against ParAlg's original word2vec model. 


Method: 
We used a set of 11,999 paraphasias produced during the Philadelphia Naming Test. We trained ParAlg with word2vec or BERT and compared their performance to humans. Finally, we evaluated BERT's performance in terms of word-sense selection and conducted an item-level discrepancy analysis to identify which aspects of semantic similarity are most challenging to classify. 


Results: 
Compared with word2vec, BERT qualitatively reduced word-sense issues and quantitatively reduced semantic classification errors by almost half. A large percentage of errors were attributable to semantic ambiguity. Of the possible semantic similarity subtypes, responses that were associated with or category coordinates of the intended target were most likely to be misclassified by both models and humans alike. 


Conclusions: 
BERT outperforms word2vec as a semantic classifier, partially due to its superior handling of polysemy. This work is an important step for further establishing ParAlg as an accurate assessment tool.},
 author = {Salem, Alexandra C. and Gale, Robert and Casilio, Marianne and Fleegle, Mikala and Fergadiotis, Gerasimos and Bedrick, Steven},
 doi = {10.1044/2022_JSLHR-22-00277},
 issn = {1092-4388, 1558-9102},
 journal = {Journal of Speech, Language, and Hearing Research},
 language = {en},
 month = {December},
 pages = {1--15},
 title = {Refining semantic similarity of paraphasias using a contextual language model},
 url = {http://pubs.asha.org/doi/10.1044/2022_JSLHR-22-00277},
 urldate = {2022-12-14},
 year = {2022}
}

@article{sanh_distilbert_2019,
 abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
 author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
 copyright = {arXiv.org perpetual, non-exclusive license},
 doi = {10.48550/ARXIV.1910.01108},
 keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
 shorttitle = {{DistilBERT}, a distilled version of {BERT}},
 title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
 url = {https://arxiv.org/abs/1910.01108},
 urldate = {2022-11-30},
 year = {2019}
}

@article{schwartz_case-series_2006,
 author = {Schwartz, M and Dell, G and Martin, N and Gahl, S and Sobel, P},
 doi = {10.1016/j.jml.2005.10.001},
 issn = {0749596X},
 journal = {Journal of Memory and Language},
 language = {en},
 month = {February},
 number = {2},
 pages = {228--264},
 shorttitle = {A case-series test of the interactive two-step model of lexical access},
 title = {A case-series test of the interactive two-step model of lexical access: evidence from picture naming},
 url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X05001051},
 urldate = {2023-01-27},
 volume = {54},
 year = {2006}
}

@article{schwartz_google_2020,
 author = {Schwartz, Barry},
 journal = {Search Engine Land},
 month = {October},
 title = {Google: {BERT} now used on almost every {English} query},
 url = {https://searchengineland.com/google-bert-used-on-almost-every-english-query-342193},
 year = {2020}
}

@article{simmons-mackie_outcome_2005,
 author = {Simmons-Mackie, Nina and Threats, Travis T. and Kagan, Aura},
 doi = {10.1016/j.jcomdis.2004.03.007},
 issn = {00219924},
 journal = {Journal of Communication Disorders},
 language = {en},
 month = {January},
 number = {1},
 pages = {1--27},
 shorttitle = {Outcome assessment in aphasia},
 title = {Outcome assessment in aphasia: {A} survey},
 url = {https://linkinghub.elsevier.com/retrieve/pii/S0021992404000346},
 urldate = {2022-12-14},
 volume = {38},
 year = {2005}
}

@book{strauss_compendium_2006,
 address = {Oxford ; New York},
 author = {Strauss, Esther and Sherman, Elisabeth M. S. and Spreen, Otfried},
 edition = {3rd ed},
 editor = {Sherman, Elisabeth M. S. and Strauss, Esther and Spreen, Otfried},
 isbn = {978-0-19-515957-8},
 keywords = {Handbooks, manuals, etc, Neuropsychological Tests, Neuropsychological tests, Reference Values},
 note = {OCLC: ocm61130794},
 publisher = {Oxford University Press},
 shorttitle = {A compendium of neuropsychological tests},
 title = {A compendium of neuropsychological tests: administration, norms, and commentary},
 year = {2006}
}

@article{tabor_parsing_1997,
 author = {Tabor, Whitney and Juliano, Cornell and Tanenhaus, Michael K.},
 doi = {10.1080/016909697386853},
 issn = {0169-0965, 1464-0732},
 journal = {Language and Cognitive Processes},
 language = {en},
 month = {March},
 number = {2-3},
 pages = {211--271},
 shorttitle = {Parsing in a dynamical system},
 title = {Parsing in a dynamical system: {An} attractor-based account of the interaction of lexical and structural constraints in sentence processing},
 url = {https://www.tandfonline.com/doi/full/10.1080/016909697386853},
 urldate = {2022-12-14},
 volume = {12},
 year = {1997}
}

@article{thompson_agrammatic_1997,
 author = {Thompson, C. K. and Lange, K. L. and Schneider, S. L. and Shapiro, L. P.},
 doi = {10.1080/02687039708248485},
 issn = {0268-7038, 1464-5041},
 journal = {Aphasiology},
 language = {en},
 month = {April},
 number = {4-5},
 pages = {473--490},
 title = {Agrammatic and non-brain-damaged subjects' verb and verb argument structure production},
 url = {http://www.tandfonline.com/doi/abs/10.1080/02687039708248485},
 urldate = {2023-02-07},
 volume = {11},
 year = {1997}
}

@article{thompson_system_1995,
 author = {Thompson, C.K. and Shapiro, L.P. and Tait, M.E. and Jacobs, B and Schneider, S.L. and Ballard, K.},
 journal = {Brain and Language},
 number = {1},
 pages = {124--129},
 title = {A system for the linguistic analysis of agrammatic language production},
 volume = {51},
 year = {1995}
}

@inproceedings{vaswani_attention_2017,
 abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
 address = {Red Hook, NY, USA},
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\textbackslash}Lukasz and Polosukhin, Illia},
 booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
 isbn = {978-1-5108-6096-4},
 note = {event-place: Long Beach, California, USA},
 pages = {6000--6010},
 publisher = {Curran Associates Inc.},
 series = {{NIPS}'17},
 title = {Attention is all you need},
 year = {2017}
}

@article{walker_short-form_2012,
 abstract = {Purpose
To create two matched short forms of the Philadelphia Naming Test (PNT; Roach, Schwartz, Martin, Grewal, \& Brecher, 1996) that yield similar results to the PNT for measuring anomia.


Method
In Study 1, archived naming data from 94 individuals with aphasia were used to identify which PNT items should be included in the short forms. The 2 constructed sets of 30 items, PNT30-A and PNT30-B, were validated using archived data from a separate group of 56 individuals with aphasia. In Study 2, the reliability of the PNT, PNT30-A, and PNT30-B across independent test administrations was evaluated with a new group of 25 individuals with aphasia who were selected to represent the full range of naming impairment.


Results
In Study 1, PNT30-A and PNT30-B were found to be internally consistent, and accuracy scores on these subsets of items were highly correlated with the full PNT. In Study 2, PNT accuracy was extremely reliable over the span of 1 week, and independent administrations of PNT30-A and PNT30-B produced similar results to the PNT and to each other.


Conclusion
The short forms of the PNT can be used to reliably estimate PNT performance, and the results can be compared to the provided norms. The 2 matched tests allow for the measurement of change in an individual’s naming ability.},
 author = {Walker, Grant M. and Schwartz, Myrna F.},
 doi = {10.1044/1058-0360(2012/11-0089)},
 issn = {1058-0360, 1558-9110},
 journal = {American Journal of Speech-Language Pathology},
 language = {en},
 month = {May},
 number = {2},
 shorttitle = {Short-{Form} {Philadelphia} {Naming} {Test}},
 title = {Short-form {Philadelphia} {Naming} {Test}: {Rationale} and empirical evaluation},
 url = {http://pubs.asha.org/doi/10.1044/1058-0360%282012/11-0089%29},
 urldate = {2022-12-14},
 volume = {21},
 year = {2012}
}

@inproceedings{wolf_transformers_2020,
 address = {Online},
 author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
 booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
 doi = {10.18653/v1/2020.emnlp-demos.6},
 language = {en},
 pages = {38--45},
 publisher = {Association for Computational Linguistics},
 shorttitle = {Transformers},
 title = {Transformers: {State}-of-the-art natural language processing},
 url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
 urldate = {2023-02-13},
 year = {2020}
}

@inproceedings{zaheer_big_2020,
 abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
 address = {Red Hook, NY, USA},
 author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
 booktitle = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
 isbn = {978-1-71382-954-6},
 note = {event-place: Vancouver, BC, Canada},
 publisher = {Curran Associates Inc.},
 series = {{NIPS}'20},
 title = {Big {Bird}: {Transformers} for longer sequences},
 year = {2020}
}
